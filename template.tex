
%% 4. 「技術研究報告」
\documentclass[technicalreport]{ieicej}
%\usepackage[dvips]{graphicx}
\usepackage[dvipdfmx]{graphicx,xcolor}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{textcomp}
\usepackage{latexsym}
%\usepackage[fleqn]{amsmath}
%\usepackage{amssymb}
\usepackage{setspace}

\jtitle{深層ニューラルネットワークを用いたDouble-Arbiter PUFに対する\\モデリング攻撃}
\jsubtitle{}
\etitle{Modeling Attacks on Double-Arbiter PUF Using Deep Neural Network}
\esubtitle{}
\authorlist{%
 %\authorentry[]{}{}{}
%\authorentry[メールアドレス]{和文著者名}{英文著者名}{所属ラベル}
  \authorentry[tiizuka@silicon.u-tokyo.ac.jp]{飯塚 知希}{Tomoki IIZUKA}{Tokyo}
  \authorentry{粟野 皓光}{Hiromitsu AWANO}{VDEC}\MembershipNumber{}
  \authorentry{池田 誠}{Makoto IKEDA}{Tokyo}\MembershipNumber{}
}
%\affiliate[]{}{}
%\affiliate[所属ラベル]{和文勤務先\\ 連絡先住所}{英文勤務先\\ 英文連絡先住所}
\affiliate[Tokyo]{東京大学工学系研究科\\ 東京都文京区本郷7-3-1}{The University of Tokyo, School of Engineering\\Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-8656, Japan}
\affiliate[VDEC]{大規模集積システム設計教育研究センター\\ 東京都文京区本郷7-3-1}{VLSI Design and Education Center\\Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-8656, Japan}
\begin{document}
%\begin{jabstract}
%%和文あらまし
%モデリング攻撃に耐性が高いとされるDouble-Arbiter PUFに対する，深層ニューラルネットを用いた攻撃手法を提案する．活性化関数にReLUを，重みの初期化手法にXavier Initializationを採用することで，約40億通りあるチャレンジ・レスポンス対のうちで，およそ0.002\%を学習させるだけで，未学習のチャレンジに対するレスポンスを88.4\%の確率で予測できることが確認された．これは従来手法を21.1\%上回る予測精度であり，急速に発展している機械学習技術による攻撃を見据えた認証スキームの設計が重要であることを明らかにした．
%\end{jabstract}
\begin{jkeyword}
%和文キーワード
PUF，ダブルアービターPUF，深層ニューラルネットワーク，機械学習攻撃，モデリング攻撃
\end{jkeyword}
\begin{eabstract}
%英文アブストラクト
A deep neural network-based modeling attack for Double-Arbiter PUF (DAPUF) is proposed. Although DAPUF is known to be highly resistant to modeling attacks, by employing some novel techniques developped in machine learning community, such as ReLU activation function and Xavier initialization technique, our model succcessfly predicted responses to unseen challenges with probability of 88.4\%, which is 21.1\% higher than the conventional method. Those results highlight an important fact that PUF-based authentication schemes should be carefully designed considering the rapid evoluving machine learning technology.
\end{eabstract}
\begin{ekeyword}
%英文キーワード
PUF, Double-Arbiter PUF, deep neural network, machine learning attack, modeling attack
\end{ekeyword}
\maketitle
%\begin{spacing}{0.92}
\section{Introduction}

In recent years IoT (Internet of Things) technology has been widely used. Such an individual identification technology at the time of communication, security measures have been attracting attention in IoT technology. Until recently, the best practice for
enabling the authentication is to store a secret key in a non-volatile
memory (NVM).  However, fabrication of a CMOS logic with dedicated NVMs requires additional processes,
which increases the device cost.  Moreover, the stored secret key is
vulnerable to physical attacks and hence the device should also be
designed with an active tamper protection/detection circuit,
which further increases the device cost.

%As a method to ensure the security, various techniques using a semiconductor device has been proposed such as storing cryptographic key data in a nonvolatile memory. However, vulnerability or when subjected to invasive attacks from the outside, the high cost of the encryption key management is an issue.

%As individual identification technology using a semiconductor device, PUF (Physically Unclonable Function) technology that generates output specific to non-replicable devices has attracted attention. PUF is applied toward the IC chip and a secure ID generation is expected. 
Physically unclonable functions (PUFs) attract increasing
attention as a promising alternative for the secret-key-based device
authentication.
When using PUFs in the authentication application, comparing each of the output response when inputting a number of challenges, a set of response and challenge registered in the authentication server, authenticating by its degree of coincidence\cite{Ulrich}. 

%Response obtained when inputting the challenge is determined by the manufacturing variation in the chip. Therefore, chips manufactured using the same mask data do not have exactly the same CRP (Challenge Response Pair), and it is impossible to duplicate by reproducing the same CRP. In recent years, manufacturing variations have increased due to miniaturization of semiconductor manufacturing process, it is useful for PUFs. 
Instead of storing a secret key in NVM, PUFs utilize a
manufacturing variability of transistors as a security key.  Due to
miniaturization of transistors, variations of a number of doping ions and/or
atomic level bumps on a gate electrode result in significant threshold
voltage ($Vth$) variations. The $Vth$ variability is an inherent
characteristic to the transistor and cannot be replicated even by the
manufacturer of the PUF, making it a unique and unclonable ``fingerprint''
of each chip.
Moreover, CRPs corresponding to the encryption key are generated every time they are used. Thus, it is not characterized as data on the chip and the storage cost of the encryption key is low\cite{Kobayashi}.

However, after gathering certain number of CRPs exchanged between the authentication server and the PUF, by modeling attack to create a model that can predict the response to unseen challenged with machine learning, it is as if artificially replicate the same PUF it has been pointed out risk. Example in which the modeling attack has already been reported for various types of PUFs, including the Arbiter PUF (APUF). Double-Arbiter PUF (DAPUF) are resistant against particular modeling attacks in PUFs, are highly safe. DAPUF is shown the results of modeling attack is difficult compared to other PUFs are, there is a problem that the technique is used in machine learning is not up-to-date in these previous studies. Technique of machine learning, especially deep learning is currently developing rapidly. Therefore. using the latest techniques, carried out a stronger attack, it is necessary to carry out the discussion of vulnerability\cite{G.Edward}.

In this study, we demonstrated a modeling attack against DAPUF, which is considered to be particularly safe among PUFs, and verified its safety. Specifically, we tried machine learning using the latest deep layer neural network, learned CRPs of DAPUF, and tried prediction of response to unseen challenge.

The results of this study are as follows.
\begin{itemize}
 \item We showed the activation function selection and weight initialization method effective for depth learning using CRPs.
 \item We achieved prediction accuracy of 88.4\% by using 0.002\% of CRPs with $ 2 ^ {32} $ (about 4 billion) in learning for DAPUF, which is generally said to be resistant to modeling attacks.
 \item Compared with the previous study using the same CRP data, it showed that the prediction accuracy is 21.1\% higher.
\end{itemize}

The structure of this paper is as follows. In Section 1, the background and the purpose of this study are described. In Section 2, after describing PUFs in detail, we show an example of previous studies of a modeling attack against PUFs. In Section 3, we describe the structure and learning method of the proposed deep layer neural network. In Section 4, we describe the result of modeling attack using the proposed method and its consideration. Finally, in Section 5, we summarize the whole and conclude.



\section{PUF and Modeling Attack}
\subsection{Physically Unclonable Function}
Fig. \ref{fig:1} shows the outline of individual authentication using PUF. First, in Characterization Phase, input a signal called challenge to PUF and obtain output signal called response. Acquire multiple pairs of CRPs that are a pair of challenge and response, and store them in the authentication server as a database. When PUFs are different, they have different CRPs due to device-specific manufacturing variations including transistor size and threshold voltage. In addition, since such manufacturing variations occur randomly in the manufacturing process, it is difficult to artificially duplicate the PUF and predict CRPs\cite{Ulrich}.

\begin{figure}[t]%fig.1
\setbox0\vbox{%
\hbox{\verb/\begin{figure}[tb]/}
\hbox{\verb/%\capwidth=60mm/}
\hbox{\verb/%\ecapwidth=60mm/}
\hbox{\verb/\vspace{45mm}/}
%\hbox{\verb/\caption{図キャプションの例}/}
\hbox{\verb/\caption{PUFによる個体認証}/}
\hbox{\verb/\label{fig:1}/}
%\hbox{\verb/\ecaption{An example of caption./}
\hbox{\verb/\ecaption{Individual authentication with PUF/}
\hbox{\verb/\end{figure}/}
}
\begin{center}
\includegraphics[width=70mm]{image/puf_outline_3.eps}
%\fbox{\box0}
\end{center}
\caption{PUFによる個体認証}
\label{fig:1}
%\ecaption{An example of caption.}
\ecaption{Individual authentication with PUF}
\end{figure}

In the Authentication Phase, challenges registered in the authentication server is input to the PUF, and authentication is performed according to whether the obtained response matches the authentication server or not.

%Various construction methods have been proposed for circuits currently used as PUFs. A typical example, APUF, is shown in Fig. \ref{fig:2}. APUF consists of two-stage circuit, and an $n$-stage selector pair (the figure shows 32 stages). The first selector pair the two input signals are simultaneously supplied. The path through which the two signals propagate is determined by the $n$-bit challenge input. If the challenge bit is ``0'', the two signals proceed without intersection, and if the challenge bit is ``1'', the two signals cross. The arrival times of the two signals are compared in the output stage, and the output bit is returned depending on whether it reached first\cite{Ulrich2}\cite{Risa}.
Various construction methods have been proposed for circuits currently used as PUFs. A typical example, APUF, is shown in Fig. \ref{fig:2}. The multiplexers (MUXes) in the APUF
pass through two input signals without changing the lanes when its
challenge bit is ``1.''  Otherwise, the signal lanes are swapped;
inputs at the top and bottom signal lanes are lead to the bottom and
top lanes, respectively.  Even though the MUXes are identically
designed, signal propagation delays of the 
straight and crossed paths are
slightly different due to manufacturing variation.  While signals
pass through the MUXes, delay difference of the two
signals is accumulated and finally converted into a binary output
using the arbiter equipped at the end of the MUXes array\cite{Ulrich2}\cite{Risa}.



\begin{figure}[t]%fig.2
\setbox0\vbox{%
\hbox{\verb/\begin{figure}[tb]/}
\hbox{\verb/%\capwidth=60mm/}
\hbox{\verb/%\ecapwidth=60mm/}
\hbox{\verb/\vspace{45mm}/}
\hbox{\verb/\caption{APUF及びDAPUFの回路図}/}
\hbox{\verb/\label{fig:2}/}
\hbox{\verb/\ecaption{Circuit diagram of APUF and DAPUF/}
\hbox{\verb/\end{figure}/}
}
\begin{center}
\includegraphics[width=85mm]{image/APUFandDAPUFmono2.eps}
%\fbox{\box0}
\end{center}
\caption{APUF及びDAPUFの回路図}
\label{fig:2}
\ecaption{Circuit diagram of APUF and DAPUF}
\end{figure}

%DAPUF is proposed as a PUF combining a plurality of APUFs. Fig. \ref{fig:2} shows a 32-stage 3-1 DAPUF combining three APUFs. Three input signals are simultaneously supplied to the first selector pair, and these paths are determined by $n$-bit challenge input. In the output stage, the arrival times are compared by the RS flip-flop, and then the exclusive OR of these results is returned as the output bit.
DAPUF is proposed as a PUF combining a plurality of APUFs. Fig. \ref{fig:2} shows a 32-stage 3-1 DAPUF combining three APUFs. Three input signals are simultaneously supplied to the first MUX, and these paths are determined by $n$-bit challenge input. In the output stage, the arrival times are compared by arbiters, and then the exclusive OR convert these result into binary output.

%Although the APUF is designed so that the upper and lower two paths have the same length, it is difficult to wire exactly the same length. It is possible to compare the paths of the same length by arranging a plurality of identical APUFs like DAPUF and comparing the paths of the upper and lower paths with the output.
DAPUF can arrange a plurality of identical APUFs and compare the paths of the upper route and the lower route with the output so as to reduce the signal propagation delays of the straight route and the crossed paths.

\subsection{Modeling Attack}
When using PUFs, the adversary pretending to be an authorized PUF is attacked by gathering CRPs and constructing an algorithm that can predict responses with high probability for any challenges. This attack is called modeling attack. From previous work, machine learning is widely known to be an effective means for modeling attack\cite{Ulrich2}.

The method of collecting CRPs required for modeling attack varies depending on the type of PUF to be attacked. Particularly in the case of PUFs such as APUF or DAPUF, it is necessary to exchange CRP between the authentication server and the PUF, and these CRPs may be gathered in large quantities. Fig. \ref{fig:7} shows the flow of modeling attack.

\begin{figure}[t]%fig.7
\setbox0\vbox{%
\hbox{\verb/\begin{figure}[tb]/}
\hbox{\verb/%\capwidth=60mm/}
\hbox{\verb/%\ecapwidth=60mm/}
\hbox{\verb/\vspace{45mm}/}
\hbox{\verb/\caption{PUFに対するモデリング攻撃}/}
\hbox{\verb/\label{fig:7}/}
\hbox{\verb/\ecaption{Modeling Attacks on PUF/}
\hbox{\verb/\end{figure}/}
}
\begin{center}
\includegraphics[width=80mm]{image/PUF_ModelingAttack2.eps}
%\fbox{\box0}
\end{center}
\caption{PUFに対するモデリング攻撃}
\label{fig:7}
\ecaption{Modeling Attacks on PUF}
\end{figure}

As a machine learning method used in modeling attack against PUF, support vector machine (SVM) is a typical example. Fig. \ref{fig:3} shows an image of pattern identification using SVM. As shown in Fig. \ref{fig:3}, it is an algorithm to classify so as to maximize the margin between different classes, and it is possible to learn the linear separation problem.

\begin{figure}[t]%fig.3
\setbox0\vbox{%
\hbox{\verb/\begin{figure}[tb]/}
\hbox{\verb/%\capwidth=60mm/}
\hbox{\verb/%\ecapwidth=60mm/}
\hbox{\verb/\vspace{45mm}/}
\hbox{\verb/\caption{SVM及びDLを用いたパターン識別}}/}
\hbox{\verb/\label{fig:3}/}
\hbox{\verb/\ecaption{Pattern identification using SVM and DL/}
\hbox{\verb/\end{figure}/}
}
\begin{center}
\includegraphics[width=85mm]{image/SVMDL.eps}
%\fbox{\box0}
\end{center}
\caption{SVM及びDLを用いたパターン識別}
\label{fig:3}
\ecaption{Pattern identification using SVM and DL}
\end{figure}
Table \ref{table:1} shows modeling attacks using SVM and logistic regression (LR) for APUF. Since the sum of the delay time differences of each stage corresponds to the delay time difference of the output, APUF can approximate accurately as a linear model. Therefore, modeling attacks using SVM are effective, and CRPs can be predicted with very high prediction rate. It can be said that the risk of being attacked is extremely high\cite{Ulrich2}\cite{Risa}.

\begin{table*}[tb]
\caption{様々なPUFに対するモデリング攻撃の結果\cite{Ulrich2}\cite{Risa}}
\label{table:1}
\ecaption{Results of modeling attacks for various PUFs\cite{Ulrich2}\cite{Risa}}
\begin{center}
\scalebox{0.8}{
 \begin{tabular}{c|c|c|c|c}
 \Hline %% ←
  Work & PUF & Machine Learning Method & Number of CRPs for Training& Prediction Rate[\%] \\
 \hline\hline
  \cite{Ulrich2} & 64 Stages APUF & LR & 18050 / $2^{64}$ (9.7×$10^{-14}$\%) & 99.9\\
 \hline
   & 64 Stages APUF & SVM & 50000 / $2^{64}$ ($2.7×10^{-13}$\%) & 96\\
 \cline{2-5}
   & 64 Stages 2-1 DAPUF & SVM & 50000 / $2^{64}$ ($2.7×10^{-13}$\%) & 87\\
 \cline{2-5}
   & 64 Stages 3-1 DAPUF & SVM & 50000 / $2^{64}$ ($2.7×10^{-13}$\%) & 58\\
 \cline{2-5}
  \cite{Risa} & 64 Stages 4-1 DAPUF & SVM & 50000 / $2^{64}$ ($2.7×10^{-13}$\%) & 56\\
 \cline{2-5}
   & 64 Stages 2-1 DAPUF & DL & 50000 / $2^{64}$ ($2.7×10^{-13}$\%) & 91\\
 \cline{2-5}
   & 64 Stages 3-1 DAPUF & DL & 50000 / $2^{64}$ ($2.7×10^{-13}$\%) & 68\\
 \cline{2-5}
   & 64 Stages 4-1 DAPUF & DL & 50000 / $2^{64}$ ($2.7×10^{-13}$\%) & 61\\
 \hline
  \cite{Ulrich2} & 64 Stages 4XOR Arbiter PUF & LR & 12000 / $2^{64}$ ($6.5×10^{-14}$\%) & 99\\
 \cline{2-5}
   & 256 Stages Ring Oscillator PUF & Quick Sort & 28891 / $2^{256}$ ($1.6×10^{-13}$\%) & 99.9\\
 \Hline %% ←
 \end{tabular}
}
\end{center}
\end{table*}
On the other hand, the DAPUF can not approximate as a linear model because it takes an exclusive OR after comparing each of the two paths of APUF with the output of another APUF. Therefore, it is difficult to perform a modeling attack using SVM, and the prediction rate is actually low as shown in Table \ref{table:1}. 

On the other hand, in DL, the gradient calculation of each parameter is iteratively performed so as to minimize the error with the target with respect to a neural network formed by combining a large number of neurons using an activation function, and the neural network is updated sequentially (Back-propagation method). By using DL, the nonlinear separation model is learned as shown in Fig. \ref{fig:3}. As shown in Table \ref{table:1}, CRPs can be predicted with higher prediction rate in DL attack compared to SVM. However, DAPUF, which combines three or more APUFs in particular, is considered to be a PUF with low precision, in other words, high safety. However, in the previous study, modeling attacks using DL, the activation function and the weight initialization method are not sufficiently studied, and only the incorporation of the noise removal auto encoder is noteworthy. Automatic encoders are not very practical at the present time, and it seems that there is much room to raise the accuracy by using other methods\cite{Risa}. 

In addition, various PUFs such as XOR Arbiter PUF which has exclusive OR of multiple APUF as output and Ring Oscillator PUF which outputs oscillation frequency of plural ring oscillators are proposed, and as shown in Table \ref{table:1}. Attack examples using various machine learning such as LR and quick sort have been reported in previous studies. However, since both PUFs achieve linearity, the prediction rate of CRP when performing a modeling attack is higher than that of DAPUF, so safety as PUF is insufficient\cite{Ulrich2}\cite{Yoshikawa}\cite{Qingqing}.

\section{Proposed Method}

\begin{figure}[t]%fig.4
\setbox0\vbox{%
\hbox{\verb/\begin{figure}[tb]/}
\hbox{\verb/%\capwidth=60mm/}
\hbox{\verb/%\ecapwidth=60mm/}
\hbox{\verb/\vspace{45mm}/}
\hbox{\verb/\caption{先行研究\cite{Risa}及び提案手法における深層ニューラルネットワーク構造}/}
\hbox{\verb/\label{fig:4}/}
\hbox{\verb/\ecaption{Deep neural network structure in previous research\cite{Risa} and proposed method/}
\hbox{\verb/\end{figure}/}
}
\begin{center}
\includegraphics[width=80mm,pagebox=cropbox,clip]{image/ProposedStudy.pdf}
%\includegraphics[width=80mm]{image/ProposedStudy3.eps}
%\fbox{\box0}
\end{center}
\caption{先行研究\cite{Risa}及び提案手法における深層ニューラルネットワーク構造}
\label{fig:4}
\ecaption{Deep neural network structure in previous research\cite{Risa} and proposed method}
\end{figure}

The detail of the deep neural network used in this study is shown in Fig. \ref{fig:4}. Unlike previous studies, since learning is performed from the input of challenges to the output of final responses end-to-end, it is considered that it is easy to acquire optimal parameters overall. Since responses of the PUF output ``0'' or ``1'', They are expressed by firing of one of the two neurons.

In the following, activation function, weight initialization, normalization method, learning method, dropout, which are necessary for designing and learning the network will be described in order.

\begin{itemize}
 \item Activation Function
\end{itemize}
~~In deep learning, it is generally possible to represent a more complicated model by increasing the number of layers. However, it becomes impossible to calculate Back-propagation, and gradient vanishes. When adopting the sigmoid function as the activation function, the maximum value of the derivative is as small as 0.25 as shown in Fig. \ref{fig:9}. Therefore, as the layer becomes deeper, a small coefficient is applied to the calculation of the error term, that will abruptly approaches zero.

In this study, ReLU (rectified linear unit), in which the gradient is in the form of a step function, learning is fast and easy to proceed, and the gradient is difficult to vanish, is adopted as the activation function. The schematic form is shown in Fig. \ref{fig:9}. In ReLU, the derivative always becomes 1, though large the input value gets. Thus, the gradient does not vanish. Moreover, since ReLU and its derivative can be represented by simple expressions without calculation of exponential function, calculation is speeded up.
In the previous study, it is necessary to divide the learning to prevent the gradient vanishing, and the weight optimization as a whole is considered insufficient. In other words, end-to-end learning can be realized by adopting ReLU, so improvement in accuracy can be expected.
Since the activation function of the output layer must be a function that outputs the probability, Softmax function is adopted.

\begin{figure}[t]%fig.9
\setbox0\vbox{%
\hbox{\verb/\begin{figure}[tb]/}
\hbox{\verb/%\capwidth=60mm/}
\hbox{\verb/%\ecapwidth=60mm/}
\hbox{\verb/\vspace{45mm}/}
\hbox{\verb/\caption{Sigmoid及びReLU関数}/}
\hbox{\verb/\label{fig:9}/}
\hbox{\verb/\ecaption{Sigmoid and ReLU function/}
\hbox{\verb/\end{figure}/}
}
\begin{center}
\includegraphics[width=70mm]{image/relusigmoid3.eps}
%\fbox{\box0}
\end{center}
\caption{Sigmoid及びReLU関数}
\label{fig:9}
\ecaption{Sigmoid and ReLU function}
\end{figure}

\begin{figure*}[t]%fig.5
\setbox0\vbox{%
\hbox{\verb/\begin{figure}[tb]/}
\hbox{\verb/%\capwidth=60mm/}
\hbox{\verb/%\ecapwidth=60mm/}
\hbox{\verb/\vspace{45mm}/}
\hbox{\verb/\caption{提案手法に基づいたモデリング攻撃の実験環境}/}
\hbox{\verb/\label{fig:5}/}
\hbox{\verb/\ecaption{Experimental environment of modeling attack based on proposed method/}
\hbox{\verb/\end{figure}/}
}
\begin{center}
%\includegraphics[width=140mm]{image/実験環境g3.eps}
\includegraphics[width=140mm,pagebox=cropbox,clip]{image/実験環境h.pdf}
%\includegraphics[bb=0 0 1125 745,width=180mm]{image/実験環境f2.png}
%\fbox{\box0}
\end{center}
\caption{提案手法に基づいたモデリング攻撃の実験環境}
\label{fig:5}
\ecaption{Experimental environment of modeling attack based on proposed method}
\end{figure*}

\begin{itemize}
 \item Weight initialization
\end{itemize}
~~Weight is initialized for learning, but it is known that learning by neural network is very sensitive to initial parameters. If they are all initialized with the same value, the slope at Back-propagation also takes the same value and the weight value is not updated, so it is preferable to initialize with random numbers close to zero. Several methods have been proposed for the probability distribution used for initialization, this study used Xavier Initialization\cite{X.Glorot}\cite{Tomoki}. This is a probability distribution in which the standard deviation is $\frac{1}{\sqrt{n}}$ when the number of nodes in the front layer is n, and it is possible to solve the gradient vanishing problem and to solve the output distribution can be avoided. By having the output distribution to have a moderate spread, it is no longer a plurality of neurons performing the same output, and improvement of the expressiveness of the network model can be expected.

\begin{itemize}
 \item Normalization method
\end{itemize}
~~This study used Batch Normalization as a method to make the output distribution of the activation function moderately broad\cite{Sergey}. Before the activation function of each layer, smoothing and scaling is performed on the input data using the mean and variance, and normalization is performed for each mini-batch used for learning. This makes it possible to avoid biasing of the output distribution in all the layers in the network and improve the expressiveness of the network model.

\begin{itemize}
 \item Learning method
\end{itemize}
~~Regarding the given challenge, parameters are updated one by one by iterative learning so that the probability that the response to unseen challenge can be correctly predicted is maximum. In this study, in order to speed up the calculation by avoiding the memory shortage, a mini batch gradient descent method, in which learning is performed after randomly selecting data for a fixed number (batch size), was used. By learning the entire sample data and then shuffling and repetition of the data again, deviation of learning is less likely to occur.

In the gradient descent method, it is common to learn by specifying a concrete value of $\eta$ = 0.001 to 0.1 for $\eta$ which is a parameter of the model, which is a learning rate. If the learning rate is made too large, the value of the neuron becomes too small in the first Back-propagation, and it becomes the same state that it does not exist on the network. If the learning rate is made too small, the learning speed will be slow and it will take time. In this study, Adam (adaptive moment estimation) which is widely used as a method for appropriately setting the learning rate was used, not by manually adjusting the learning rate\cite{X.Glorot}. This makes it possible to expect efficient and high-accuracy learning without falling into a local optimal solution.

\begin{itemize}
 \item Over learning and Dropout
\end{itemize}
~~In the neural network, over training learns only training data complicatedly, and the model which becomes a pattern classification different from the actual data distribution becomes a problem. In this study, ensemble learning was demonstrated to generate multiple models by randomly dropping out neurons with constant probability every learning. This makes it possible to improve the generalization performance of the model.

\section{Numerical experiment}
\subsection{Experiment setting}

The flow of this experiment is shown in Fig. \ref{fig:5}. In this experiment, 32 stages 3 - 1 DAPUF was used as an attack target. First, virtually 32 stages of 3-1 DAPUF were created and responses to challenges were obtained by transistor level simulation. In this simulation, 65 nm PTM\cite{Nanoscale} was used for the transistor model, and the manufacturing variation was simulated by varying the threshold voltage according to the normal random number. The Pelgom coefficient was set based on the evaluation result in the 65 nm process\cite{S. Saxena}.

Second, 100,000 pairs are randomly selected as training data from these CRPs and machine learning is performed using deep neural networks. This is equivalent to 0.002\% of $2^{32}$ (about 4 billion) CRPs.

As shown in the figure, the number of hidden layers is four, and the number of neurons is set to 32 → 4000 → 8000 → 4000 → 32 → 2 in order from the input layer. The dropout probability was 0.2 for the input layer only, and 0.5 for the other layers, which are generally chosen values\cite{Srivastava}. In learning, the batch size was set to 128, and iterative learning of 41 epochs was performed until the minimization of the error function was completed. After the learning was completed, the prediction rate of the responses to the unlearned challenges is calculated by using 60 thousands of CRPs as the test data.

\subsection{Experimental Result}
Experimental results by the proposed method are shown in Fig. \ref{fig:6}. Prediction rate of training data and test data were improved according to the progress of learning, and finally the prediction rate of unlearned test data reached 88.4\%.

\begin{figure}[t]%fig.6
\setbox0\vbox{%
\hbox{\verb/\begin{figure}[tb]/}
\hbox{\verb/%\capwidth=60mm/}
\hbox{\verb/%\ecapwidth=60mm/}
\hbox{\verb/\vspace{45mm}/}
\hbox{\verb/\caption{提案手法による32段DAPUFに対するモデリング攻撃の結果}/}
\hbox{\verb/\label{fig:6}/}
\hbox{\verb/\ecaption{Result of modeling attack on 32-stage DAPUF by proposed method/}
\hbox{\verb/\end{figure}/}
}
\begin{center}
%\includegraphics[bb=0 0 1074 822,width=70mm]{image/18011117061.png}
\includegraphics[width=70mm]{image/18011117062.eps}
%\fbox{\box0}
\end{center}
\caption{提案手法による32段DAPUFに対するモデリング攻撃の結果}
\label{fig:6}
\ecaption{Result of modeling attack on 32-stage DAPUF by proposed method}
\end{figure}

Next, using the same data as CRPs used in this experiment, the method of the previous study was accurately reproduced and compared with the proposed method. The results are shown in Table 2. In the previous study, the prediction rate was only 67.3\%, the result was 21.1\% higher than the proposed method.

\begin{table}[tb]
\caption{先行研究\cite{Risa}及び提案手法による32段DAPUFに対するモデリング攻撃の結果}
\label{table:2}
\ecaption{Result of modeling attack on 32-stage DAPUF by previous research\cite{Risa} and proposed method}
\begin{center}
 \begin{tabular}{c|c}
 \Hline %% ←
    & Prediction Rate[\%] \\
 \hline
  Previous research & 67.3 \\
 \hline
  Proposed method & 88.4\\
 \Hline %% ←
 \end{tabular}
\end{center}
\end{table}

\subsection{Consideration}
Fig. \ref{fig:8} shows the relationship between the number of CRPs used for authentication and the probability of discriminating imitations in the previous study and the proposed method. The horizontal axis shows the number of CRPs to be compared with the authentication server at the time of authentication, and the vertical axis shows the probability of being able to prevent the modeling attack. In this figure, it is assumed that the influence of environmental fluctuation is not taken into consideration, and the genuine PUF always shows the same characteristics as CRPs of the authentication server.

As an example, considering discrimination of imitations with a probability of 99\% or more, in the case of the modeling attack by the previous study, if the authentication is performed using 12 or more sets of CRPs, it is possible to discriminate imitations by a modeling attack. On the other hand, in the proposed method, more than 38 sets of CRPs are required to discriminate imitations. In other words, when the modeling attack using the proposed method is received compared to the previous study, since the probability of discrimination of imitations is low, it is necessary to raise the security level of DAPUF by increasing the number of CRPs used for authentication.

\begin{figure}[t]%fig.8
\setbox0\vbox{%
\hbox{\verb/\begin{figure}[tb]/}
\hbox{\verb/%\capwidth=60mm/}
\hbox{\verb/%\ecapwidth=60mm/}
\hbox{\verb/\vspace{45mm}/}
\hbox{\verb/\caption{CRPの数と模造品を判別できる確率の関係}/}
\hbox{\verb/\label{fig:8}/}
\hbox{\verb/\ecaption{Relationship between the number of CRPs and the probability of discriminating imitations/}
\hbox{\verb/\end{figure}/}
}
\begin{center}
%\includegraphics[bb=0 0 479 335,width=70mm]{image/consideration.png}
\includegraphics[width=70mm]{image/consideration3.eps}
%\fbox{\box0}
\end{center}
\caption{CRPの数と模造品を判別できる確率の関係}
\label{fig:8}
\ecaption{Relationship between the number of CRPs and the probability of discriminating imitations}
\end{figure}


\section{Conclusion}
In this paper, we demonstrated the modeling attack based on the latest deep layer neural network against DAPUF which is considered to be highly resistant to modeling attack among PUFs. We adopted Xavier Initialization as weight initialization method and ReLU as activation function respectively and demonstrated deep learning using 0.002\% of $2^{32}$ (about 4 billion) CRPs.
As a result, responses to unseen challenges could be predicted with accuracy of 88.4\%, 21.1\% higher than the previous study. We clarified that it is important to design an authentication schemes should be carefully designed considering the rapid evolving machine learning technology. 


\subsubsection*{Acknowledgment}
The authors acknowledge support from VDEC with the collaboration with Synopsys Corporation.

%\end{spacing}

%\bibliographystyle{sieicej}
%\bibliography{myrefs}
\begin{thebibliography}{99}% 文献数が10未満の時 {9}
\bibitem{Ulrich} Ulrich Ruhrmair, and Daniel E. Holcomb, ”PUFs at a Glance,” Design, Automation and Test in Europe Conference and Exhibition, 2014.

\bibitem{Kobayashi} 小林 行雄，”半導体で実現する低コスト/高信頼のIoTセキュリティ，”マイナビニュース，2018年1月5日.

\bibitem{G.Edward} G.Edward Suh et al, ”Physical Unclonable Functions for Device Authentication and Secret Key Generation,” DAC2007, 2007.

\bibitem{Ulrich2} Ulrich Ruhrmair et al, ”Modeling Attacks on Physical Unclonable Functions,” CCS’10, 2010.

\bibitem{Risa} Risa Yashiro et al, ”Deep-Learning-Based Security Evaluation on Authetication Systems Using Arbiter PUF and Its Variants,” IWSEC2016, LNCS9836, pp.267-285, 2016.

\bibitem{Yoshikawa} 吉川 雅弥 et al，”統計補正処理を用いた経路選択リングオシレータPUFとその実装評価，”システム制御情報学会誌，Vol.25，No.1，pp.1-9，2012.

\bibitem{Qingqing} Qingqing Chen et al, ”The Bistable Ring PUF: A New Architecture for Strong Physical Unclonable Functions,” IEEE International Symposium on Hardware-Oriented Security and Trust, 2011.

\bibitem{X.Glorot} X. Glorot, and Y. Bengio, ”Understanding the difficulty of training deep feedforward neural networks,” Proc.AISTATS, Vol.9, pp249-256, 2010.

\bibitem{Tomoki} 中丸 智貴 et al，”多層ニューラルネットワークのパラメータ初期化手法の修正，”生産研究，Vol.68，No.3，pp.261-264，2016.

\bibitem{Sergey} Sergey Ioffe, and Christian Szegedy, ”Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,” arXiv:1502.03167v3 [cs.LG], 2015.

\bibitem{Nanoscale} Nanoscale Integration and Modeling (NIMO) Group, ASU, “Predictive
Technology Model (PTM),” http://ptm.asu.edu/.

\bibitem{S. Saxena} S. Saxena et al., “Variation in transistor performance and leakage in
nanometer-scale technologies,” IEEE Trans. Electron Devices, vol. 55,
no. 1, pp. 131?144, Jan. 2008.

\bibitem{Srivastava} Srivastava, et al., ”Dropout: A simple way to prevent neural networks from overfitting,” The Journal of Machine Learning Reserch, Vol.15, No.1, pp.1929-1958, 2014.


\end{thebibliography}

\end{document}
